import pandas as pd
import numpy as np
import pylab as pl
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import precision_score
from sklearn.metrics import Logistic Regression.
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import statsmodels.api as sm

dataset = pd.read_csv(c\desktop\sensordata)
dataset.columns = ['id','time','R1','R2','R3','R4','R5','R6','R7','R8','Temp.','Humidity']
dataset.set_index('id',inplace = True)
dataset.head()
output = pd.read_csv('data/HT_Sensor_metadata.csv',sep = '\t',header = None)
output.columns = ['id','date','class','t0','dt']
output.head()

dataset = dataset.join(output,how = 'inner')
dataset.set_index(np.arange(dataset.shape[0]),inplace = True)
dataset['time']  += dataset['t0']
dataset.drop(['t0'],axis = 1,inplace=True)
dataset.head()
fig, axes = plt.subplots(nrows=3, ncols=2)#, sharex=True, sharey=True)
fig.set_figheight(20)
fig.set_figwidth(25)
fig.subplots_adjust(hspace=.5)
axes[0,0].plot(dataset.time[dataset.id == 3],dataset.R1[dataset.id == 3],c = 'red',linewidth = '2.0')
axes[0,0].set_title('R1 Vs Time')
axes.head();
axes[0,0].set_xlabel('Time(hour)')
axes[0,0].set_ylabel('R1_values(kilo ohm)')
axes[0,1].plot(dataset.time[dataset.id == 3],dataset.R2[dataset.id == 3],c = 'green',linewidth = '2.0')
axes[0,1].set_title('R2 Vs Time')
axes[0,0].set_xlabel('Time(hour)')
axes[0,0].set_ylabel('R2_values (kilo ohm)')
axes[1,0].plot(dataset.time[dataset.id == 3],dataset.R3[dataset.id == 3],c = 'orange',linewidth = '2.0',label = 'R3 (Sensor)')
#axes[1,0].set_title('R3 Vs Time')
axes[1,0].set_xlabel('Time(hour)')
axes[1,0].set_ylabel('R3_values (kilo ohm)')
axes[1,0].plot(dataset.time[dataset.id == 3],dataset.R4[dataset.id == 3],c = 'blue',linewidth = '2.0',label = 'R4')
axes[1,0].set_title('R4 and R3 Vs Time')
axes[1,0].set_xlabel('Time(hour)')
axes[1,0].set_ylabel('Reading (kilo ohm)')
axes[1,0].legend(loc = 4)
axes.head()

axes[1,1].plot(dataset.time[dataset.id == 3],dataset.R5[dataset.id == 3],c = 'pink',linewidth = '2.0')
axes[1,1].set_title('R5 Vs Time')
axes[1,1].set_xlabel('Time(hour)')
axes[1,1].set_ylabel('R5_values (kilo ohm)')
 

axes[2,0].plot(dataset.time[dataset.id == 3],dataset.R6[dataset.id == 3],c = 'violet',linewidth = '2.0',label = 'R6')
#axes[2,0].set_title('R6 Vs Time')
axes[2,0].set_xlabel('Time(hour)')
axes[2,0].set_ylabel('R6_values (kilo ohm)')


axes[2,0].plot(dataset.time[dataset.id == 3],dataset.R7[dataset.id == 3],c = 'black',linewidth = '2.0',label ='R7')
axes[2,0].set_title('R7 and R6 Vs Time')
axes[2,0].set_xlabel('Time(hour)')
axes[2,0].set_ylabel('Reading (kilo ohm)')
axes[2,0].legend()

axes[2,1].plot(dataset.time[dataset.id == 3],dataset.R8[dataset.id == 3],c = 'brown',linewidth = '2.0')
axes[2,1].set_title('R8 Vs Time')
axes[2,1].set_xlabel('Time(hour)')
axes[2,1].set_ylabel('R8_values (kilo ohm)')
plt.suptitle('Sensor Reading on Day 3')
pl.savefig("Graph1.png", dpi=300)
fig, axes = plt.subplots(nrows=1, ncols=2)#, sharex=True, sharey=True)
axes[0].plot(dataset.time[dataset.id == 17],dataset['Temp.'][dataset.id == 17],c = 'r')
axes[0].set_title('R1 Vs Temp')
axes[0].set_xlabel('Time (hour)')
axes[0].set_ylabel('Temprature (C)')
axes[1].plot(dataset.time[dataset.id == 17],dataset.Humidity[dataset.id == 17],c = 'green')
axes[1].set_title('R2 Vs Humidity')
axes[1].set_xlabel('Humidity (%)')
plt.suptitle('Temprature Reading on Day 3')
pl.savefig("Graph2.png", dpi=300)
dataset.corr()
dataset.corr() > 0.98
xtrain_dataframe = pd.DataFrame(xtrain)
ytrain_dataframe = pd.DataFrame(ytrain)
xtest_dataframe = pd.DataFrame(xtest)
ytest_dataframe = pd.DataFrame(ytest)
print(ytest_dataframe)
xtrain_dataframe.columns = [u'R1',u'R2',u'R3',u'R4',u'R5',u'R6',u'R7',u'R8',u'Temp.',u'Humidity']
ytrain_dataframe.columns = ['class']
xtest_dataframe.columns = [u'R1',u'R2',u'R3',u'R4',u'R5',u'R6',u'R7',u'R8',u'Temp.',u'Humidity']
ytest_dataframe.columns = ['class']
res = sm.RLM(ytrain_dataframe, xtrain_dataframe).fit()
res.summary()
res.head()
xtrain,xtest,ytrain,ytest = train_test_split(dataset[[u'R2',u'R3',u'R4',u'R5',u'R6',u'R7',u'R8',u'Temp.',u'Humidity']].values,dataset['class'].values,train_size = 0.7)
for i in range(ytrain.shape[0]):
    if(ytrain[i] == 'background'):
        ytrain[i] = 0
    elif(ytrain[i] == 'banana'):
        ytrain[i] = 1
    else:
        ytrain[i] = 2
        
for i in range(ytest.shape[0]):
    if(ytest[i] == 'background'):
        ytest[i] = 0
    elif(ytest[i] == 'banana'):
        ytest[i] = 1
    else:
        ytest[i] = 2
        
ytrain = ytrain.astype('int64')
ytest = ytest.astype('int64')
ytest.head()
 
# Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.

from sklearn.cross_validation import KFold
Cs = [0.001,0.01,0.1,1,10,100]
Print(Cs)
count = 0
score_test = []
score_train = []
for c in Cs:
    score1 = []
    score2 = []
    est = LogisticRegression(C=c,n_jobs= 4)
    for itrain,itest in KFold(xtrain.shape[0],11):
        est.fit(xtrain[itrain],ytrain[itrain])
        score1.append(accuracy_score(est.predict(xtrain[itest]),ytrain[itest]))
score1.head()
        score2.append(accuracy_score(est.predict(xtrain[itrain]),ytrain[itrain]))
    score_test.append(np.mean(score1))
    score_train.append(np.mean(score2))



plt.plot([0.001,0.01,0.1,1,10,100],score_train,'o-',label = 'training_score')
plt.plot([0.001,0.01,0.1,1,10,100],score_test,'o-',label = 'testing_score')
plt.xscale('log')
plt.legend(loc = 4)
plt.xlabel('Values of Regularization Parameter')
plt.ylabel('Accuracy Score')
plt.axhline(y=score_train[1],c = 'black')
plt.title('Prediction Acurracy of Logistic Regression')
pl.savefig("Graph3.png", dpi=300)
est = LogisticRegression(C = Cs[np.argmax(score)],n_jobs = 4)
est.fit(xtrain,ytrain)
ypred = est.predict(xtest)
confusion_matrix(ytest, ypred)
define accuracy
accuracy=(ypred==85%)
print(accuracy)

from sklearn.svm import SVC

C_2d_range = [1e-2]
gamma_2d_range = [1e-1]
classifiers = []
for C in C_2d_range:
    for gamma in gamma_2d_range:
        clf = SVC(C=C, gamma=gamma)
        clf.fit(xtrain, ytrain)
        classifiers.append((C, gamma, clf))

accuracy_score(clf.predict(xtest),ytest)

# As we can see the accuray is much high for training set. Let us check the accuracy of SVM for training set

accuracy_score(clf.predict(xtrain),ytrain)

# #### Hence we have approached our desired accuracy which is better than the paper we followed. 
# Training Set Accuracy 96.22%.  
# Testing Set Accuracy 96.18%

# # We achieve more accuracy than the paper.

